{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Classification Using PyTorch and CMLE.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    }
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "text",
        "id": "UWvrShlZjZwr"
      },
      "cell_type": "markdown",
      "source": [
        "# Text Classification Using PyTorch and CMLE\n",
        "\n",
        "This notebook illustrates CMLE's new custom prediction feature. It allows us to execute arbitrary python pre-processing code prior to invoking a model, as well as post-processing on the produced predictions. In addition, you can use a model build by your favourite python-based framework!\n",
        "\n",
        "This is all done server-side so that the client can pass data directly to CMLE in the unprocessed state.\n",
        "\n",
        "We will build a text classification model using PyTorch, while performing text preproessing using Keras"
      ]
    },
    {
      "metadata": {
        "id": "PEuI7Hw5PquP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r733GnVjSwgp",
        "colab_type": "code",
        "outputId": "1c6b2d77-2534-4d02-f56f-d4a6620a4a09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==1.12 torch "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.12\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bd/68/ec26b2cb070a5760707ec8d9491a24e5be72f4885f265bb04abf70c0f9f1/tensorflow-1.12.0-cp27-cp27mu-manylinux1_x86_64.whl (83.1MB)\n",
            "\u001b[K    100% |████████████████████████████████| 83.1MB 252kB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python2.7/dist-packages (1.0.1.post2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.12) (1.15.0)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.12) (2.0.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.12) (1.0.7)\n",
            "Requirement already satisfied: enum34>=1.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.12) (1.1.6)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.12) (3.7.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.12) (1.0.9)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.12) (0.2.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.12) (0.33.1)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.12) (0.7.1)\n",
            "Requirement already satisfied: backports.weakref>=1.0rc1 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.12) (1.0.post1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.12) (1.11.0)\n",
            "Collecting tensorboard<1.13.0,>=1.12.0 (from tensorflow==1.12)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/ae/9840c4837c6f54034ac942b5344396e8c3d74686a9bd29beafdf633cc221/tensorboard-1.12.2-py2-none-any.whl (3.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 3.1MB 7.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.12) (1.14.6)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.12) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.12) (0.7.1)\n",
            "Requirement already satisfied: futures>=2.2.0 in /usr/local/lib/python2.7/dist-packages (from grpcio>=1.8.6->tensorflow==1.12) (3.2.0)\n",
            "Requirement already satisfied: funcsigs>=1; python_version < \"3.3\" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow==1.12) (1.0.2)\n",
            "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow==1.12) (5.1.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python2.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.12) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python2.7/dist-packages (from protobuf>=3.6.1->tensorflow==1.12) (40.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python2.7/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.12) (0.14.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python2.7/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.12) (3.0.1)\n",
            "Installing collected packages: tensorboard, tensorflow\n",
            "  Found existing installation: tensorboard 1.13.1\n",
            "    Uninstalling tensorboard-1.13.1:\n",
            "      Successfully uninstalled tensorboard-1.13.1\n",
            "  Found existing installation: tensorflow 1.13.1\n",
            "    Uninstalling tensorflow-1.13.1:\n",
            "      Successfully uninstalled tensorflow-1.13.1\n",
            "Successfully installed tensorboard-1.12.2 tensorflow-1.12.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "bupGswGOcMW2",
        "outputId": "aba75458-3190-4d34-f9f5-b8a2f9ac3bb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import torch\n",
        "print(tf.__version__) \n",
        "print(torch.__version__) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.estimator package not installed.\n",
            "tf.estimator package not installed.\n",
            "1.12.0\n",
            "1.0.1.post2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "eQWHJ3X7vLeQ"
      },
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "66JlKmfzvPhN",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ewdEsJuVvNqm",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "PROJECT='vijays-sandbox' # CHANGE TO YOUR GCP PROJECT NAME\n",
        "BUCKET='vijays-sandbox-ml' # CHANGE TO YOUR GCS BUCKET NAME\n",
        "ROOT='torch_text_classification'\n",
        "MODEL_DIR=os.path.join(ROOT,'models')\n",
        "PACKAGES_DIR=os.path.join(ROOT,'packages')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Qe49zwLLSXzu",
        "colab_type": "code",
        "outputId": "58d8081d-b58e-494b-e504-065a768a9a36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "# Delete any previous artefacts from Google Cloud Storage\n",
        "!gsutil rm -r gs://{BUCKET}/{ROOT}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Removing gs://vijays-sandbox-ml/torch_text_classification/models/processor_state.pkl#1551360019520520...\n",
            "Removing gs://vijays-sandbox-ml/torch_text_classification/models/torch_saved_model.pt#1551360016536407...\n",
            "Removing gs://vijays-sandbox-ml/torch_text_classification/packages/my_package-0.1.tar.gz#1551360065067790...\n",
            "/ [3 objects]                                                                   \n",
            "Operation completed over 3 objects.                                              \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "2utHS2nnzKty",
        "outputId": "e303d0bc-d6bf-4a4c-9402-d7f237144974",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!gcloud config set project {PROJECT}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "V12s5BFCkYhC"
      },
      "cell_type": "markdown",
      "source": [
        "## Download and Explore Data"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ww6dAQ7EkHzZ",
        "outputId": "af58faac-c405-4e14-a644-a7f54dfa08c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "gsutil cp gs://cloud-training-demos/blogs/CMLE_custom_prediction/keras_text_pre_processing/train.tsv .\n",
        "gsutil cp gs://cloud-training-demos/blogs/CMLE_custom_prediction/keras_text_pre_processing/eval.tsv ."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://cloud-training-demos/blogs/CMLE_custom_prediction/keras_text_pre_processing/train.tsv...\n",
            "/ [0 files][    0.0 B/  4.0 MiB]                                                \r/ [1 files][  4.0 MiB/  4.0 MiB]                                                \r\n",
            "Operation completed over 1 objects/4.0 MiB.                                      \n",
            "Copying gs://cloud-training-demos/blogs/CMLE_custom_prediction/keras_text_pre_processing/eval.tsv...\n",
            "/ [0 files][    0.0 B/  1.4 MiB]                                                \r/ [1 files][  1.4 MiB/  1.4 MiB]                                                \r\n",
            "Operation completed over 1 objects/1.4 MiB.                                      \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "g1XEyk4toH76",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!head eval.tsv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "bHqYoDlFsw02"
      },
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "yIarI3u3r6vd"
      },
      "cell_type": "markdown",
      "source": [
        "### Pre-processing class to be used in both training and serving"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "GdkNipQtr8-P",
        "outputId": "8dd22601-e4f6-4a2d-9be6-22f7fe94c493",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "%%writefile preprocess.py\n",
        "\n",
        "from tensorflow.python.keras.preprocessing import sequence\n",
        "from tensorflow.keras.preprocessing import text\n",
        "\n",
        "class TextPreprocessor(object):\n",
        "  def __init__(self, vocab_size, MAX_SEQUENCE_LENGTH):\n",
        "    self._vocabb_size = vocab_size\n",
        "    self._MAX_SEQUENCE_LENGTH = MAX_SEQUENCE_LENGTH\n",
        "    self._tokenizer = None\n",
        "\n",
        "  def fit(self, text_list):        \n",
        "    # Create vocabulary from input corpus.\n",
        "    tokenizer = text.Tokenizer(num_words=self._vocabb_size)\n",
        "    tokenizer.fit_on_texts(text_list)\n",
        "    self._tokenizer = tokenizer\n",
        "\n",
        "  def transform(self, text_list):        \n",
        "    # Transform text to sequence of integers\n",
        "    text_sequence = self._tokenizer.texts_to_sequences(text_list)\n",
        "\n",
        "    # Fix sequence length to max value. Sequences shorter than the length are\n",
        "    # padded in the beginning and sequences longer are truncated\n",
        "    # at the beginning.\n",
        "    padded_text_sequence = sequence.pad_sequences(\n",
        "      text_sequence, maxlen=self._MAX_SEQUENCE_LENGTH)\n",
        "    return padded_text_sequence"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing preprocess.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "pEk_bHo08a1n"
      },
      "cell_type": "markdown",
      "source": [
        "### Test Prepocessing Locally"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "sKtHuVUw8kFO",
        "outputId": "0d881cc9-b898-42a6-b5c9-6faaf79d4da4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from preprocess import TextPreprocessor\n",
        "\n",
        "processor = TextPreprocessor(5, 5)\n",
        "processor.fit(['hello machine learning'])\n",
        "processor.transform(['hello machine learning'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 1, 2, 3]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "lsTGmBn4s2fC"
      },
      "cell_type": "markdown",
      "source": [
        "## Model Creation"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "eFKEHCk_WYQU"
      },
      "cell_type": "markdown",
      "source": [
        "### Metadata"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "P-1_lAwsWNFf",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "CLASSES = {'github': 0, 'nytimes': 1, 'techcrunch': 2}  # label-to-int mapping\n",
        "NUM_CLASSES = 3\n",
        "VOCAB_SIZE = 20000  # Limit on the number vocabulary size used for tokenization\n",
        "MAX_SEQUENCE_LENGTH = 50  # Sentences will be truncated/padded to this length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "jkzmfu3jV-78"
      },
      "cell_type": "markdown",
      "source": [
        "### Prepare data for training and evaluation"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "HDX_3jquWCBY",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from preprocess import TextPreprocessor\n",
        "\n",
        "def load_data(train_data_path, eval_data_path):\n",
        "    # Parse CSV using pandas\n",
        "    column_names = ('label', 'text')\n",
        "    \n",
        "    df_train = pd.read_csv(train_data_path, names=column_names, sep='\\t')\n",
        "    df_train = df_train.sample(frac=1)\n",
        "    \n",
        "    df_eval = pd.read_csv(eval_data_path, names=column_names, sep='\\t')\n",
        "\n",
        "    return ((list(df_train['text']), np.array(df_train['label'].map(CLASSES))),\n",
        "            (list(df_eval['text']), np.array(df_eval['label'].map(CLASSES))))\n",
        "\n",
        "\n",
        "((train_texts, train_labels), (eval_texts, eval_labels)) = load_data(\n",
        "       'train.tsv', 'eval.tsv')\n",
        "\n",
        "# Create vocabulary from training corpus.\n",
        "processor = TextPreprocessor(VOCAB_SIZE, MAX_SEQUENCE_LENGTH)\n",
        "processor.fit(train_texts)\n",
        "\n",
        "# Preprocess the data\n",
        "train_texts_vectorized = processor.transform(train_texts)\n",
        "eval_texts_vectorized = processor.transform(eval_texts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "r9sDfoXesC1d"
      },
      "cell_type": "markdown",
      "source": [
        "### Build the model"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "E7hifM2esEe9",
        "outputId": "ca199fd7-bb34-4cee-c1fa-c69bbc663380",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "%%writefile torch_model.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TorchTextClassifier(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_dim, seq_length, num_classes, \n",
        "                 num_filters, kernel_size, pool_size, dropout_rate):\n",
        "        super(TorchTextClassifier, self).__init__()\n",
        "\n",
        "        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
        "        \n",
        "        self.conv1 = nn.Conv1d(seq_length, num_filters, kernel_size)\n",
        "        self.max_pool1 = nn.MaxPool1d(pool_size)\n",
        "        self.conv2 = nn.Conv1d(num_filters, num_filters*2, kernel_size)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.dense = nn.Linear(num_filters*2, num_classes)\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = self.embeddings(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.max_pool1(x)\n",
        "        \n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool1d(x, x.size()[2]).squeeze(2)\n",
        "        \n",
        "        x = self.dropout(x)\n",
        "        x = self.dense(x)\n",
        "        x = F.softmax(x, 1)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing torch_model.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "53Gef0A4sjdV"
      },
      "cell_type": "markdown",
      "source": [
        "### Train and save the model"
      ]
    },
    {
      "metadata": {
        "id": "PCkmyHfASX0g",
        "colab_type": "code",
        "outputId": "354cb0fc-bbcc-452a-cf50-17bf0fd5b0b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "\n",
        "LEARNING_RATE=.001\n",
        "FILTERS=64\n",
        "DROPOUT_RATE=0.2\n",
        "EMBEDDING_DIM=200\n",
        "KERNEL_SIZE=3\n",
        "POOL_SIZE=3\n",
        "\n",
        "NUM_EPOCH=1\n",
        "BATCH_SIZE=128\n",
        "\n",
        "train_size = len(train_texts)\n",
        "steps_per_epoch = int(len(train_labels)/BATCH_SIZE)\n",
        "\n",
        "print(\"Train size: {}\".format(train_size))\n",
        "print(\"Batch size: {}\".format(BATCH_SIZE))\n",
        "print(\"Number of epochs: {}\".format(NUM_EPOCH))\n",
        "print(\"Steps per epoch: {}\".format(steps_per_epoch))\n",
        "print(\"Vocab Size: {}\".format(VOCAB_SIZE))\n",
        "print(\"Embed Dimensions: {}\".format(EMBEDDING_DIM))\n",
        "print(\"Sequence Length: {}\".format(MAX_SEQUENCE_LENGTH))\n",
        "print(\"\")\n",
        "\n",
        "\n",
        "def get_batch(step):\n",
        "    start_index = step*BATCH_SIZE\n",
        "    end_index = start_index + BATCH_SIZE\n",
        "    x = Variable(torch.Tensor(train_texts_vectorized[start_index:end_index]).long())\n",
        "    y = Variable(torch.Tensor(train_labels[start_index:end_index]).long())\n",
        "    return x, y\n",
        "\n",
        "\n",
        "from torch_model import TorchTextClassifier\n",
        "\n",
        "model = TorchTextClassifier(VOCAB_SIZE, \n",
        "                            EMBEDDING_DIM, \n",
        "                            MAX_SEQUENCE_LENGTH, \n",
        "                            NUM_CLASSES, \n",
        "                            FILTERS, \n",
        "                            KERNEL_SIZE, \n",
        "                            POOL_SIZE, \n",
        "                            DROPOUT_RATE)\n",
        "\n",
        "model.train()\n",
        "loss_metric = F.cross_entropy\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "for epoch in range(NUM_EPOCH):\n",
        "\n",
        "    for step in range(steps_per_epoch):\n",
        "\n",
        "        x, y = get_batch(step)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        y_pred = model(x)\n",
        "\n",
        "        loss = loss_metric(y_pred, y) \n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % 50 == 0:\n",
        "            print('Batch [{}/{}] Loss: {}'.format(step+1, steps_per_epoch, round(loss.item(),5)))\n",
        "\n",
        "\n",
        "    print('Epoch [{}/{}] Loss: {}'.format(epoch+1, NUM_EPOCH, round(loss.item(),5)))\n",
        "\n",
        "print('Final Loss: {}'.format(epoch+1, NUM_EPOCH, round(loss.item(),5)))\n",
        "\n",
        "torch.save(model, 'torch_saved_model.pt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train size: 72162\n",
            "Batch size: 128\n",
            "Number of epochs: 1\n",
            "Steps per epoch: 563\n",
            "Vocab Size: 20000\n",
            "Embed Dimensions: 200\n",
            "Sequence Length: 50\n",
            "\n",
            "Batch [1/563] Loss: 1.11598\n",
            "Batch [51/563] Loss: 1.07404\n",
            "Batch [101/563] Loss: 1.08879\n",
            "Batch [151/563] Loss: 1.05261\n",
            "Batch [201/563] Loss: 1.05945\n",
            "Batch [251/563] Loss: 1.00419\n",
            "Batch [301/563] Loss: 0.98083\n",
            "Batch [351/563] Loss: 0.95064\n",
            "Batch [401/563] Loss: 0.94694\n",
            "Batch [451/563] Loss: 0.89967\n",
            "Batch [501/563] Loss: 0.86751\n",
            "Batch [551/563] Loss: 0.94085\n",
            "Epoch [1/1] Loss: 0.86216\n",
            "Final Loss: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "MakbLwTbuMsZ"
      },
      "cell_type": "markdown",
      "source": [
        "### Save pre-processing object\n",
        "\n",
        "We need to save this so the same tokenizer used at training can be used to pre-process during serving"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "sziwQgs0uZzx",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('./processor_state.pkl', 'wb') as f:\n",
        "  pickle.dump(processor, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "4AWJZP3stCta"
      },
      "cell_type": "markdown",
      "source": [
        "## Custom Model Prediction Preparation"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "KeR_jDYjuymX"
      },
      "cell_type": "markdown",
      "source": [
        "### Copy model and pre-processing object to GCS"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "OJwsuGK3ub4S",
        "outputId": "ec1facd0-b502-4fc6-d41a-52fffaf3bb56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "!gsutil cp torch_saved_model.pt gs://{BUCKET}/{MODEL_DIR}/\n",
        "!gsutil cp processor_state.pkl gs://{BUCKET}/{MODEL_DIR}/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying file://torch_saved_model.pt [Content-Type=application/octet-stream]...\n",
            "-\n",
            "Operation completed over 1 objects/15.4 MiB.                                     \n",
            "Copying file://processor_state.pkl [Content-Type=application/octet-stream]...\n",
            "/ [1 files][  3.3 MiB/  3.3 MiB]                                                \n",
            "Operation completed over 1 objects/3.3 MiB.                                      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "DZ0H1GKAueAp"
      },
      "cell_type": "markdown",
      "source": [
        "### Define Model Class"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "xLvpSsMiufVr",
        "outputId": "4f9aee42-7c50-45f4-f667-72ee55f051e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "%%writefile model_prediction.py\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "class CustomModelPrediction(object):\n",
        "  def __init__(self, model, processor):\n",
        "    self._model = model\n",
        "    self._processor = processor\n",
        "\n",
        "  def _postprocess(self, predictions):\n",
        "    labels = ['github', 'nytimes', 'techcrunch']\n",
        "    label_indexes = [np.argmax(prediction) \n",
        "                     for prediction in predictions.detach().numpy()]\n",
        "    return [labels[label_index] for label_index in label_indexes]\n",
        "\n",
        "\n",
        "  def predict(self, instances, **kwargs):\n",
        "    preprocessed_data = self._processor.transform(instances)\n",
        "    predictions =  self._model(Variable(torch.Tensor(preprocessed_data).long()))\n",
        "    labels = self._postprocess(predictions)\n",
        "    return labels\n",
        "\n",
        "\n",
        "  @classmethod\n",
        "  def from_path(cls, model_dir):\n",
        "    import torch \n",
        "    import torch_model\n",
        "    model = torch.load(os.path.join(model_dir,'torch_saved_model.pt'))\n",
        "    model.eval()\n",
        "    with open(os.path.join(model_dir, 'processor_state.pkl'), 'rb') as f:\n",
        "      processor = pickle.load(f)\n",
        "\n",
        "    return cls(model, processor)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing model_prediction.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "kL-jv3GDg_zD"
      },
      "cell_type": "markdown",
      "source": [
        "### Test Model Class Locally"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ynL_ovR32Od0",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Headlines for Predictions\n",
        "\n",
        "techcrunch=[\n",
        "  'Uber shuts down self-driving trucks unit',\n",
        "  'Grover raises €37M Series A to offer latest tech products as a subscription',\n",
        "  'Tech companies can now bid on the Pentagon’s $10B cloud contract'\n",
        "]\n",
        "nytimes=[\n",
        "  '‘Lopping,’ ‘Tips’ and the ‘Z-List’: Bias Lawsuit Explores Harvard’s Admissions',\n",
        "  'A $3B Plan to Turn Hoover Dam into a Giant Battery',\n",
        "  'A MeToo Reckoning in China’s Workplace Amid Wave of Accusations'\n",
        "]\n",
        "github=[\n",
        "  'Show HN: Moon – 3kb JavaScript UI compiler',\n",
        "  'Show HN: Hello, a CLI tool for managing social media',\n",
        "  'Firefox Nightly added support for time-travel debugging'\n",
        "]\n",
        "requests = (techcrunch+nytimes+github)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "zx53zlPK6YIK",
        "outputId": "ec8bcefd-786e-4fc4-d485-f5c0f25438b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "cell_type": "code",
      "source": [
        "from model_prediction import CustomModelPrediction\n",
        "\n",
        "model = CustomModelPrediction.from_path('.')\n",
        "model.predict(requests)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['nytimes',\n",
              " 'techcrunch',\n",
              " 'techcrunch',\n",
              " 'nytimes',\n",
              " 'github',\n",
              " 'nytimes',\n",
              " 'github',\n",
              " 'github',\n",
              " 'github']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Q4BEQqoGyZP1"
      },
      "cell_type": "markdown",
      "source": [
        "### Package up files and copy to GCS"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "DgQ9UJG_u6Jk",
        "outputId": "f7fed653-37f5-4f0f-ce3b-23d92e0e6705",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "%%writefile setup.py\n",
        "\n",
        "from setuptools import setup\n",
        "\n",
        "REQUIRED_PACKAGES = ['torch', 'keras']\n",
        "\n",
        "setup(\n",
        "  name=\"my_package\",\n",
        "  version=\"0.1\",\n",
        "  include_package_data=True,\n",
        "  scripts=[\"preprocess.py\", \"model_prediction.py\", \"torch_model.py\"],\n",
        "  install_requires=REQUIRED_PACKAGES\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing setup.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "IjwWftmpybCI",
        "outputId": "5ba3cbba-6110-464a-9844-7cd7578debc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        }
      },
      "cell_type": "code",
      "source": [
        "!python setup.py sdist\n",
        "!gsutil cp ./dist/my_package-0.1.tar.gz gs://{BUCKET}/{PACKAGES_DIR}/my_package-0.1.tar.gz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running sdist\n",
            "running egg_info\n",
            "creating my_package.egg-info\n",
            "writing requirements to my_package.egg-info/requires.txt\n",
            "writing my_package.egg-info/PKG-INFO\n",
            "writing top-level names to my_package.egg-info/top_level.txt\n",
            "writing dependency_links to my_package.egg-info/dependency_links.txt\n",
            "writing manifest file 'my_package.egg-info/SOURCES.txt'\n",
            "reading manifest file 'my_package.egg-info/SOURCES.txt'\n",
            "writing manifest file 'my_package.egg-info/SOURCES.txt'\n",
            "warning: sdist: standard file not found: should have one of README, README.rst, README.txt, README.md\n",
            "\n",
            "running check\n",
            "warning: check: missing required meta-data: url\n",
            "\n",
            "warning: check: missing meta-data: either (author and author_email) or (maintainer and maintainer_email) must be supplied\n",
            "\n",
            "creating my_package-0.1\n",
            "creating my_package-0.1/my_package.egg-info\n",
            "copying files to my_package-0.1...\n",
            "copying model_prediction.py -> my_package-0.1\n",
            "copying preprocess.py -> my_package-0.1\n",
            "copying setup.py -> my_package-0.1\n",
            "copying torch_model.py -> my_package-0.1\n",
            "copying my_package.egg-info/PKG-INFO -> my_package-0.1/my_package.egg-info\n",
            "copying my_package.egg-info/SOURCES.txt -> my_package-0.1/my_package.egg-info\n",
            "copying my_package.egg-info/dependency_links.txt -> my_package-0.1/my_package.egg-info\n",
            "copying my_package.egg-info/requires.txt -> my_package-0.1/my_package.egg-info\n",
            "copying my_package.egg-info/top_level.txt -> my_package-0.1/my_package.egg-info\n",
            "Writing my_package-0.1/setup.cfg\n",
            "creating dist\n",
            "Creating tar archive\n",
            "removing 'my_package-0.1' (and everything under it)\n",
            "Copying file://./dist/my_package-0.1.tar.gz [Content-Type=application/x-tar]...\n",
            "/ [1 files][  1.9 KiB/  1.9 KiB]                                                \n",
            "Operation completed over 1 objects/1.9 KiB.                                      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "pU4D8prVtLNI"
      },
      "cell_type": "markdown",
      "source": [
        "## Model Deployment to CMLE"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "WUEA9FKcy8fM",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "MODEL_NAME='torch_text_classification'\n",
        "VERSION_NAME='v201903'\n",
        "RUNTIME_VERSION='1.12'\n",
        "REGION='us-central1'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "JBao9v7e1OU0",
        "outputId": "af4b8219-f9c5-4f8f-d9a6-b0acf561f7de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "!gcloud ml-engine models create {MODEL_NAME} --regions {REGION}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;31mERROR:\u001b[0m (gcloud.ml-engine.models.create) Resource in project [vijays-sandbox] is the subject of a conflict: Field: model.name Error: A model with the same name already exists.\n",
            "- '@type': type.googleapis.com/google.rpc.BadRequest\n",
            "  fieldViolations:\n",
            "  - description: A model with the same name already exists.\n",
            "    field: model.name\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "YpZkN4o71PUY",
        "outputId": "4937b721-664c-42e9-e769-215ac7405432",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "!gcloud ml-engine versions delete {VERSION_NAME} --model {MODEL_NAME} --quiet # run if version already created"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;31mERROR:\u001b[0m (gcloud.ml-engine.versions.delete) NOT_FOUND: Field: name Error: The specified model version was not found.\n",
            "- '@type': type.googleapis.com/google.rpc.BadRequest\n",
            "  fieldViolations:\n",
            "  - description: The specified model version was not found.\n",
            "    field: name\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "WbE2cKVE1PaX",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!gcloud alpha ml-engine versions create {VERSION_NAME} --model {MODEL_NAME} \\\n",
        "--origin=gs://{BUCKET}/{MODEL_DIR}/ \\\n",
        "--python-version=2.7 \\\n",
        "--runtime-version={RUNTIME_VERSION} \\\n",
        "--framework='SCIKIT_LEARN' \\\n",
        "--package-uris=gs://{BUCKET}/{PACKAGES_DIR}/my_package-0.1.tar.gz \\\n",
        "--machine-type=mls1-c4-m4 \\\n",
        "--model-class=model_prediction.CustomModelPrediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Jn6EFbPUzUTm"
      },
      "cell_type": "markdown",
      "source": [
        "## Online Predictions from CMLE"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "mNFvql_-zC4N",
        "outputId": "38101952-d6e7-4714-9366-705535670a7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "from googleapiclient import discovery\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import json\n",
        "\n",
        "# JSON format the requests\n",
        "request_data = {'instances': requests}\n",
        "\n",
        "# Authenticate and call CMLE prediction API \n",
        "credentials = GoogleCredentials.get_application_default()\n",
        "api = discovery.build('ml', 'v1', credentials=credentials,\n",
        "            discoveryServiceUrl='https://storage.googleapis.com/cloud-ml/discovery/ml_v1_discovery.json')\n",
        "\n",
        "parent = 'projects/{}/models/{}/versions/{}'.format(PROJECT, MODEL_NAME, VERSION_NAME)\n",
        "print(\"Model full name: {}\".format(parent))\n",
        "response = api.projects().predict(body=request_data, name=parent).execute()\n",
        "\n",
        "print(response['predictions'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model full name: projects/vijays-sandbox/models/torch_text_classification/versions/v201903\n",
            "[u'nytimes', u'techcrunch', u'techcrunch', u'nytimes', u'github', u'nytimes', u'github', u'github', u'github']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Pm3d3buc0Bi1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## License"
      ]
    },
    {
      "metadata": {
        "id": "CA7T5kIZ0Bsq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Authors: Khalid Salama & Vijay Reddy \n",
        "\n",
        "---\n",
        "**Disclaimer**: This is not an official Google product. The sample code provided for an educational purpose.\n",
        "\n",
        "---\n",
        "\n",
        "Copyright 2019 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0.\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License."
      ]
    },
    {
      "metadata": {
        "id": "kRHsJH6R0Geb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}