{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2019 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Training\n",
    "This notebook trains a model to predict whether the given sonar signals are bouncing off a metal cylinder or off a cylindrical rock from [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+%28Sonar%2C+Mines+vs.+Rocks%29).\n",
    "\n",
    "## The data\n",
    "The Sonar Signals dataset that this sample uses for training is provided by the UC Irvine Machine Learning Repository. Google has hosted the data on a public GCS bucket `gs://cloud-samples-data/ml-engine/sonar/sonar.all-data`.\n",
    "\n",
    "* `sonar.all-data` is split for both training and evaluation\n",
    "\n",
    "Note: Your typical development process with your own data would require you to upload your data to GCS so that you can access that data from inside your notebook. However, in this case, Google has put the data on GCS to avoid the steps of having you download the data from UC Irvine and then upload the data to GCS.\n",
    "\n",
    "### Disclaimer\n",
    "This dataset is provided by a third party. Google provides no representation, warranty, or other guarantees about the validity or any other aspects of this dataset.\n",
    "\n",
    "# Build your model\n",
    "First, you'll create the model (provided below). This is similar to your normal process for creating a PyTorch model. However, there is one key difference:\n",
    "\n",
    "Downloading the data from GCS at the start of your file, so that you can access the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# Add code to download the data from GCS (in this case, using the publicly hosted data).\n",
    "# you will then be able to use the data when training your model.\n",
    "# ---------------------------------------\n",
    "# Public bucket holding the census data\n",
    "bucket = storage.Client().bucket('cloud-samples-data')\n",
    "\n",
    "# Path to the data inside the public bucket\n",
    "blob = bucket.blob('ml-engine/sonar/sonar.all-data')\n",
    "# Download the data\n",
    "blob.download_to_filename('sonar.all-data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# Read in the data\n",
    "# ---------------------------------------\n",
    "\n",
    "# Define the dataset to be used by PyTorch\n",
    "class SonarDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.dataframe = pd.read_csv(csv_file, header=None)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # When iterating through the dataset get the features and targets\n",
    "        features = self.dataframe.iloc[idx, :-1].values.astype(dtype='float64')\n",
    "\n",
    "        # Convert the targets to binary values:\n",
    "        # R = rock --> 0\n",
    "        # M = mine --> 1\n",
    "        target = self.dataframe.iloc[idx, -1:].values\n",
    "        if target[0] == 'R':\n",
    "            target[0] = 0\n",
    "        elif target[0] == 'M':\n",
    "            target[0] = 1\n",
    "        target = target.astype(dtype='float64')\n",
    "\n",
    "        # Load the data as a tensor\n",
    "        data = {'features': torch.from_numpy(features),\n",
    "                'target': target}\n",
    "        return data\n",
    "\n",
    "\n",
    "# Load the data\n",
    "sonar_dataset = SonarDataset('./sonar.all-data')\n",
    "# Create indices for the split\n",
    "dataset_size = len(sonar_dataset)\n",
    "test_size = int(0.2 * dataset_size)  # Use a test_split of 0.2\n",
    "train_size = dataset_size - test_size\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, test_dataset = random_split(sonar_dataset,\n",
    "                                           [train_size, test_size])\n",
    "# Create our Dataloaders for training and test data\n",
    "train_loader = DataLoader(\n",
    "    train_dataset.dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset.dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# This is where your model code would go. Below is an example model using the census dataset.\n",
    "# ---------------------------------------\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Create the Deep Neural Network\n",
    "class SonarDNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SonarDNN, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(60, 60),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(60, 30),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(30, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Create the model\n",
    "net = SonarDNN().double()\n",
    "optimizer = optim.SGD(net.parameters(),\n",
    "                      lr=0.01,\n",
    "                      momentum=0.5,\n",
    "                      nesterov=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# Define the training loop\n",
    "# ---------------------------------------\n",
    "def train(net, train_loader, optimizer, epoch):\n",
    "    \"\"\"Create the training loop\"\"\"\n",
    "    net.train()\n",
    "    criterion = nn.BCELoss()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_index, data in enumerate(train_loader):\n",
    "        features = data['features']\n",
    "        target = data['target']\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(features)\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if batch_index % 6 == 5:  # print every 6 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch, batch_index + 1, running_loss / 6))\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# Define the testing loop\n",
    "# ---------------------------------------\n",
    "def test(net, test_loader):\n",
    "    \"\"\"Test the DNN\"\"\"\n",
    "    net.eval()\n",
    "    criterion = nn.BCELoss()  # https://pytorch.org/docs/stable/nn.html#bceloss\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader, 0):\n",
    "            features = data['features']\n",
    "            target = data['target']\n",
    "            output = net(features)\n",
    "            # Binarize the output\n",
    "            pred = output.apply_(lambda x: 0.0 if x < 0.5 else 1.0)\n",
    "            test_loss += criterion(output, target)  # sum up batch loss\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set:\\n\\tAverage loss: {:.4f}'.format(test_loss))\n",
    "    print('\\tAccuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            correct,\n",
    "            (len(test_loader) * test_loader.batch_size),\n",
    "            100. * correct / (len(test_loader) * test_loader.batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# Train / Test the model\n",
    "# ---------------------------------------\n",
    "epochs = 10\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(net, train_loader, optimizer, epoch)\n",
    "    test(net, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# Export the trained model\n",
    "# ---------------------------------------\n",
    "torch.save(net.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -al model.pth"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
